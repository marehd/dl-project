(this["webpackJsonpdl-site"]=this["webpackJsonpdl-site"]||[]).push([[0],{22:function(e,t,o){},23:function(e,t,o){},45:function(e,t,o){"use strict";o.r(t);var a=o(0),s=o(1),n=o.n(s),r=o(12),i=o.n(r),h=(o(22),o(13)),l=o(14),d=o(2),c=o(16),u=o(15),p=(o.p,o(23),o(4)),b=o.n(p),g=function(e){Object(c.a)(o,e);var t=Object(u.a)(o);function o(e){var s;return Object(h.a)(this,o),(s=t.call(this,e)).state={startingText:"Default text",textType:"default",comp:[Object(a.jsx)(b.a,{speed:20,children:Object(a.jsx)("span",{id:"text",children:"Generate a shower thought"})})],goodThoughts:["The world would be a better place if every country had an equal number of billionaires","The 'cool' part of shopping is choosing the right item to fit your budget.","Some people\u2019s life skills are probably useless because no one will ever see them in action.","The biggest reason your dog or cat likes you is because you're with them.","Everyone talks about how old people are but it\u2019s a lot younger than the average person.","The only good thing about the old school cars is that they've a lot more power to operate than a light-powered car.","The average age of a human in the galaxy has probably never been lower than it has been in the past.","When I was younger, I had an ice castle in the mountains. The Ice castle would have been a real castle if there had been Ice Knights in the Middle Ages, because it was the best ice castle in the world.","If you want to get a degree, you must not work at a job which could make you the boss","The most successful people in a business are probably always the ones that use money to pay for themselves.","The entire world is literally a drug problem.","People who use google for information are often actually trying to find information that's good.","Maybe some people don't exist?","The first person to take over the world would be the person with the highest degree of skill in the world.","The first person to take over the world would be the world's greatest success story.","The first person to take over the world would be the world's most wealthy person, so much money would be better for everyone, but the second person wouldn't share.","If every bullet missed the target it would still be bulletproof",'In a couple hundred years or so, "Google Street View" will mean something really dumb like riding across the street trying to find a parking space, or standing awkwardly next to a car with no space to breathe.',"A few months back Google was going to unveil a new version of Chrome called Google Explorer.","I've always wanted a life on the outside of the Earth but never the inside.","I feel better when I'm alone than when I am alone with a partner.","We don't believe in ghosts or aliens and think they're just some mysterious force that is constantly trying to abduct people","The more you learn about science, the easier it is for science teachers to convince students that their knowledge is actually valuable.","What if the only reason we didn\u2019t notice aliens in the first place was because they didn\u2019t like the first humans?","If humans had evolved to avoid extinction, there\u2019d be a lot less trash in our dumps.",'"It\'s Game Over" has no way of stopping people from saying the game is cruel.',"What if every cell in your body is constantly watching you, making decisions that affect your life on a daily basis, while your mind is just waiting for something bad to happen to fix it","Your body is basically a brain keeping you alive","If you traveled back in time and invented something, you'd probably get a lot of flak from the past.","Everyone would be interested in a new Pixar movie if the movie opened with a montage of the adventures of a bunch of Pixar employees trying to figure out what their next movie was.","If you live to be 100 you probably miss the 90s. If you live to be 150 you probably miss the 2000s.",'We think of life in terms of "how?" but not "why?"',"If aliens didn't attack Earth, maybe people would just buy souvenirs from them, and use them.","If you tell a baby in a language you don\u2019t speak, they\u2019ll never know it. It\u2019s like they just thought the word sounded the same in both languages.","In 20 years we'll all be telling children they don't have to wear pants to school because classes might be online","When I ask my kids to show me how to make something they like, they'll usually act in a way that makes them look like an expert.","When you buy a coffee, you don\u2019t necessarily want to taste it first but rather smell it.","It'd be nice if Google Maps had a \"don't miss a beat\" button that shows where to park your vehicle.","If the internet keeps people anonymous, then being a nice person on the internet is pretty good indicator that you\u2019re a good person.","In the future, the world will be flooded with robots and AI that will kill us in a matter of seconds","When you're sick, you can take a nap. When you're old, you can take a break.","Every kid out there probably thinks the world is a better place because of a lack of money and social interactions","If someone tells you to watch your back, they don't want you thinking you\u2019re the badass. If someone tells you not to fight, they want you thinking you\u2019re the badass","As a kid I was excited about the idea of flying cars, now that cars are mainstream I worry about flying houses.","My dog doesn't have a pet but when I'm sick, I wish it had one","As a kid, I used to think that everyone was a little crazy, and that the people I knew were the real crazy. But then the internet got really interesting, and I realized, everybody is just so crazy."],badThoughts:["My dog probably reads all the things he sees whenever he walks through my house.",'The word "hebopatch" has more words for the same species that hebopatch.',"The word 'pitching' has the word 'cut' on it.","Being a father of two and a father of three is a bit like the middle-aged man eating all his Oreos but finding nothing.",'We should put an ad in the news that starts with "Newsflash - An animal was killed by a giant robot." It\'ll make our news more relevant.',"The person who invented lollipops probably never wore one.","In video games you don't kill other people to get the loot you desire. You loot other people for wanting more of your loot.","You would never be able to differentiate between a good dog and a bad dog","If your country is at war you are technically a human being","All the best people have lived their lives with zero fear of a zombie apocalypse.","I'm glad we have air conditioning because it basically keeps us alive longer","What if in Star Wars, the original stormtroopers fought in the final battle, and the Sith Lords had no idea who their opponents were?",'There should be a sub called "The Roadrunner" so all the bad guys could stop and be nice to each other.',"If you think a tree is a vampire, and you eat it, you're not a vampire, you're a cannibal","There's no reason you shouldn't have your own fridge.","The first person to have seen a spider was the only one who ever thought of opening a can of water","The human eye can see through clouds but the human nose cannot see through dust.","What if the world is flat and we just made this world a simulation?","As I get older I get more and more convinced that my generation is the one that created porn, not pornstars.","If a dog says 'I'm sorry', is it the dog's fault for saying it to you?","There is probably no one better than Tom Cruise, who should have been a billionaire before he became one of the most successful serial killers ever","If you had a kid, you would've probably seen him eat a lot of Oreos.","If people ever make it to space, we will have to wait until after the 2020 Olympics so everyone will have the chance to prove that they can rocket to the moon with zero gravity."]},s.generateGood=s.generateGood.bind(Object(d.a)(s)),s.generateBad=s.generateBad.bind(Object(d.a)(s)),s}return Object(l.a)(o,[{key:"generateGood",value:function(){var e=this,t=this.state.goodThoughts.length,o=Math.floor(Math.random()*t);this.setState({comp:[Object(a.jsx)("div",{})]},(function(){return e.setState({comp:[Object(a.jsx)(b.a,{speed:15,children:Object(a.jsx)("span",{id:"text",children:e.state.goodThoughts[o]})})]})}))}},{key:"generateBad",value:function(){var e=this,t=this.state.badThoughts.length,o=Math.floor(Math.random()*t);this.setState({comp:[Object(a.jsx)("div",{})]},(function(){return e.setState({comp:[Object(a.jsx)(b.a,{speed:15,children:Object(a.jsx)("span",{id:"text",children:e.state.badThoughts[o]})})]})}))}},{key:"render",value:function(){return Object(a.jsxs)("div",{className:"App",children:[Object(a.jsxs)("div",{id:"top-bar",children:[Object(a.jsx)("a",{href:"https://marehd.github.io/dl-project/",children:Object(a.jsx)("img",{id:"logo",src:"data:image/png;base64,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"})}),Object(a.jsx)("h1",{children:"r/ShowerThoughtGenerator"})]}),Object(a.jsxs)("div",{id:"generator",children:[Object(a.jsxs)("section",{id:"generator-title",children:[Object(a.jsx)("h2",{children:"Generator"}),Object(a.jsxs)("div",{id:"button",children:[Object(a.jsx)("button",{onClick:this.generateGood,children:"+ Generate good"}),Object(a.jsx)("button",{onClick:this.generateBad,children:"+ Generate bad"})]})]}),Object(a.jsx)("div",{id:"text-container",children:this.state.comp[0]})]}),Object(a.jsxs)("div",{className:"box",children:[Object(a.jsx)("h2",{children:"Abstract"}),Object(a.jsx)("p",{children:"Creativity and imagination are thought to be two of the traits most unique to humanity. One such bastion of these two traits is the subreddit r/ShowerThoughts. On this subreddit, users post so-called \u201cshower thoughts\u201d - insights that might strike one while carrying out a routine task like taking a shower. Examples include \u201cYour stomach thinks all potatoes are mashed\u201d or \u201cAnything is a UFO if you\u2019re stupid enough\u201d. Given the creativity of these types of posts, we thought it would be interesting to attempt to replicate results from this subreddit."}),Object(a.jsx)("p",{children:"We have produced a language model using GPT-2 to generate potential posts. We trained this model using a dataset composed of all the posts on the subreddit with more than a thousand karma. To create this model, we ran a small grid search over two hyperparameters, the learning rate and epsilon, and selected the best-performing ones. We found these to be an epsilon of 1e-8 and a learning rate of 5e-5. We also determined that training on 3 epochs using a batch size of 2 was optimal. Our final results led to a test perplexity of approximately 1.13. We found that our model generated many interesting \u201cshower thoughts\u201d, potentially showing that creativity may not be as unique as we initially thought."})]}),Object(a.jsxs)("div",{className:"box",children:[Object(a.jsx)("h2",{children:"Problem statement"}),Object(a.jsx)("p",{children:"Our goal was to produce a language model that generated realistic reddit posts for a specific subreddit, capable of fooling humans. To make things more interesting, we decided to base our experiments on r/Showerthoughts, given that producing creative and original shower thoughts is a difficult task."})]}),Object(a.jsxs)("div",{className:"box",children:[Object(a.jsx)("h2",{children:"Related work"}),Object(a.jsx)("p",{children:"There are two subreddits which aim to complete tasks similar to our model. The first of these two is r/SubredditSimulator, a subreddit on which Markov chains are used to replicate the content of other subreddits. Markov chains are simpler than our model which uses transformers, as each word in a Markov chain is dependent only on the last few words before it. This has the unfortunate side effect of the last parts of sentences bearing almost no relation to the beginnings of said sentences. For example, one post from the subreddit r/SubredditSimulator has the title \u201cFlorida Man refuses to pay child support despite DNA test proving he is \u2018going to jail for being slow\u2019\u201d. As we can see, the end of the sentence refers to going to jail for being slow while the beginning refers to child support. "}),Object(a.jsx)("p",{children:"The second subreddit that bears a resemblance to our model is  r/SubSimulatorGPT2, which is a subreddit where all the submissions and comments are generated by fine-tuned GPT-2 bots. While the model used by this subreddit bears much more of a resemblance to our model, this subreddit is less popular than r/SubredditSimulator. Since we are going to use GPT-2, a transformer-based model, we are going to introduce how transformers were introduced as a way to improve recurrent neural networks. Recurrent neural networks (RNNs) use hidden states and attention so that the last few words in a sentence can depend on all the words that came prior. Although an improvement when compared to Markov chains, they carry the disadvantage of needing the first words to be processed before the last words, which we know sometimes isn\u2019t the case while writing a sentence. For instance, consider noun-noun compounds such as bus stop, here we know that bus is describing the type of stop it is, instead of being the head of the phrase. Therefore, transformers were introduced to address the limitations of RNNs."}),Object(a.jsx)("p",{children:"Transformers rely heavily on attention and don't require sentences to be processed in order.  This makes it such that  training can be parallelized, which reduces training times. An article in towardsdatascience.com, How Neural Networks Are Learning To Write, does a deeper comparison of Markov chains, RNNs and Transformers and their use in generating reddit posts. Hence, since Transformers are the state of the art in natural language generation, and the usage of GPT-2 (Transformer based) in subreddits leads to reasonable and logical content, we are going to fine-tune GPT-2 with posts from the r/ShowerThoughts subreddit."}),Object(a.jsx)("p",{children:"For our dataset, we initially considered training using posts from the subreddit r/TwoSentenceHorror.. However, after inspecting the available posts we realized that there weren\u2019t enough high quality posts that we felt could lead to interesting results. Additionally, several posts contained vulgar, explicit and offensive language. Therefore we decided to use the subreddit r/ShowerThoughts as our training dataset since it contained a larger number of high quality posts with fewer NSFW posts as a result of stricter subreddit moderation. In order to get this dataset we used the PushShift API and its Python wrapper psaw to scrape the subreddit. We found this API more useful than the official Reddit API since it didn\u2019t have a limit of posts per request. Using psaw and pandas, we collected every post that had a score greater than 1000. We also filtered the posts further by excluding any posts that contained NSFW language and we ended up with a total of 14,846 posts."})]}),Object(a.jsxs)("div",{className:"box",children:[Object(a.jsx)("h2",{children:"Methodology"}),Object(a.jsx)("p",{children:"Initially, we tried simulating posts from the subreddit r/TwoSentenceHorror using an LSTM. However, we found that there were many NSFW and offensive posts in this subreddit due to more lax moderation policies. This made it difficult to filter out posts without making our model biased. After some unsuccessful attempts at generating SFW posts using an LSTM, we changed our approach and instead opted to train our model on r/Showerthoughts. We learned that r/ShowerThoughts had moderation policies that prohibited posts containing political or potentially offensive content. As a result, the posts on this subreddit were of much higher quality."}),Object(a.jsx)("p",{children:"To generate our dataset of posts from r/Showerthoughts, we crawled the subreddit and extracted all posts with more than 1000 karma, a proxy for popularity. Our final dataset had 14,846 total posts, which we then  randomly split into 12846/1000/1000 for training, validation and testing."}),Object(a.jsx)("p",{children:"However, after some preliminary experiments using an LSTM we realized that our approach wasn\u2019t robust enough and wasn\u2019t capturing interesting dependencies in the data. Further, a lot of the generated sentences contained grammatical mistakes. Therefore we instead opted to fine-tune GPT-2 on our dataset"}),Object(a.jsx)("p",{children:"To fine-tune GPT-2 on our web-crafted dataset, we first tokenized all posts using the GPT-2 tokenizer, padding sentences up to a maximum length of 512 tokens. We then added custom start and end of text tokens to each of the posts prior to training."}),Object(a.jsx)("p",{children:"Following this, we ran a small grid search for 1 epoch to determine the best values for the learning rate and epsilon for the AdamW optimizer, and used these results in our experiments. We then used 100 warmup steps with a lower learning rate, and then defaulted to fine-tuning the model using a learning rate of 5e-5 for the remaining of the training process. In order to avoid out-of-memory errors while training, we used a relatively small batch size of 2."}),Object(a.jsx)("p",{children:"We fine-tuned the model for 5 epochs. As shown in the plots below, the model began to overfit the data after epoch 3. We therefore used the model\u2019s state after epoch 3 to generate our sentences and plot the test perplexity. Fine-tuning the model for 5 epochs took approximately 4 hours on an NVIDIA T4 GPU with 16 Gb of graphics memory."})]}),Object(a.jsxs)("div",{className:"box",children:[Object(a.jsx)("h2",{children:"Experiments/evaluation"}),Object(a.jsx)("p",{children:"Our experiments consisted of fine-tuning GPT-2 on our dataset of the best posts from the subreddit r/Showerthoughts, and generating sentences using the resulting model. We trained the model for one through five epochs, and used the one with the lowest validation loss as our final model. Given that it was difficult to judge the performance of the model exclusively from the sentences produced, we also plotted the train and validation perplexities per epoch as well as the final test loss."})]}),Object(a.jsxs)("div",{className:"box",children:[Object(a.jsx)("h2",{children:"Results"}),Object(a.jsx)("p",{children:"Our results indicate that early stopping after three epochs achieves a test perplexity very similar to the validation perplexity, and that the model starts overfitting the training data after epoch three. However, in a language model that generates creative shower thoughts, the best way of evaluating our results is by generating some sentences. Here are some nice examples that our model produced:"}),Object(a.jsxs)("ul",{children:[Object(a.jsx)("li",{children:"The world would be a better place if every country had an equal number of billionaires"}),Object(a.jsx)("li",{children:"The 'cool' part of shopping is choosing the right item to fit your budget."}),Object(a.jsx)("li",{children:"The only reason your dog or cat likes you is because you're with them."}),Object(a.jsx)("li",{children:"Some people\u2019s life skills are probably useless because no one will ever see them in action"}),Object(a.jsx)("li",{children:"When you're sick, you can take a nap. When you're old, you can take a break."}),Object(a.jsxs)("li",{children:["In the future, the world will be flooded with robots and AI that will kill us in a matter of seconds ",Object(a.jsx)("span",{className:"comment",children:"(we should\u2019ve probably been nicer to our AWS instance)"})]})]}),Object(a.jsxs)("p",{children:["Furthermore, we posted the top sentence in the subreddit r/Showerthoughts. Take a look at ",Object(a.jsx)("a",{children:"this"})," and ",Object(a.jsx)("a",{children:"this"})," link to view them on reddit!"]}),Object(a.jsx)("p",{children:"However, not all sentences generated by our model were coherent. Oftentimes, our model would spit out grammatically correct sentences that resemble clever shower thoughts, but without much intrinsic meaning. Here are some examples of that:"}),Object(a.jsxs)("ul",{children:[Object(a.jsx)("li",{children:"My dog probably reads all the things he sees whenever he walks through my house."}),Object(a.jsx)("li",{children:"I'm glad we have air conditioning because it basically keeps us alive longer"}),Object(a.jsx)("li",{children:"What if in Star Wars, the original stormtroopers fought in the final battle, and the Sith Lords had no idea who their opponents were?"}),Object(a.jsx)("li",{children:"The person who invented lollipops probably never wore one."}),Object(a.jsx)("li",{children:'There should be a law that allows your spouse to use a "no contact, ask for a divorce" sign.'})]}),Object(a.jsx)("p",{children:"Although some of these sentences are quite funny, they don\u2019t necessarily reflect creative or fun thoughts. Nevertheless, all sentences generated have a \u201cshower thought\u201d structure, namely short sentences reflecting hypothetical and/or odd scenarios that few people might\u2019ve thought about."})]}),Object(a.jsx)("div",{id:"bottom-bar"})]})}}]),o}(s.Component),f=function(e){e&&e instanceof Function&&o.e(3).then(o.bind(null,46)).then((function(t){var o=t.getCLS,a=t.getFID,s=t.getFCP,n=t.getLCP,r=t.getTTFB;o(e),a(e),s(e),n(e),r(e)}))};i.a.render(Object(a.jsx)(n.a.StrictMode,{children:Object(a.jsx)(g,{})}),document.getElementById("root")),f()}},[[45,1,2]]]);
//# sourceMappingURL=main.43cdfa6a.chunk.js.map